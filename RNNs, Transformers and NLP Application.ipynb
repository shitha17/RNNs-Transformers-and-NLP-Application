{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IyTdMiIpUZhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5RM7iFjUSTAM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41BA6axUSUJS",
        "outputId": "5e4a0a53-bb25-4be1-cf83-6089e99e1050"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  7318k      0  0:00:11  0:00:11 --:--:-- 15.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBqSxoLjS8V0",
        "outputId": "fe62e516-513b-4241-e3b6-300bb7437e33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzNHzlFATCz4",
        "outputId": "466b17c6-fa31-4a96-902c-5bce93740da2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN46nCqZTKjs",
        "outputId": "ed06bec8-24a8-4701-a151-c5e80bcdba39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "6Eu6NRiMTO82"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating training,validation and test datasets\n",
        "batch_size = 32\n",
        "raw_train_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"batches in raw_training_dataset is: {raw_train_data.cardinality()}\")\n",
        "print(f\"batches in raw_validation_dataset is: {raw_val_data.cardinality()}\")\n",
        "print(f\"batches in raw_test_dataset is: {raw_test_data.cardinality()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIF2MdeA-LXI",
        "outputId": "25b00326-9551-48ad-f46b-3059d4d5396f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "batches in raw_training_dataset is: 625\n",
            "batches in raw_validation_dataset is: 157\n",
            "batches in raw_test_dataset is: 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_data.take(2):\n",
        "    for i in range(6):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52Z64Iu-TA7",
        "outputId": "c517ee61-468d-4fc7-f6ee-de519a8f1344"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'I\\'ve seen tons of science fiction from the 70s; some horrendously bad, and others thought provoking and truly frightening. Soylent Green fits into the latter category. Yes, at times it\\'s a little campy, and yes, the furniture is good for a giggle or two, but some of the film seems awfully prescient. Here we have a film, 9 years before Blade Runner, that dares to imagine the future as somthing dark, scary, and nihilistic. Both Charlton Heston and Edward G. Robinson fare far better in this than The Ten Commandments, and Robinson\\'s assisted-suicide scene is creepily prescient of Kevorkian and his ilk. Some of the attitudes are dated (can you imagine a filmmaker getting away with the \"women as furniture\" concept in our oh-so-politically-correct-90s?), but it\\'s rare to find a film from the Me Decade that actually can make you think. This is one I\\'d love to see on the big screen, because even in a widescreen presentation, I don\\'t think the overall scope of this film would receive its due. Check it out.'\n",
            "1\n",
            "b'First than anything, I\\'m not going to praise I\\xc3\\xb1arritu\\'s short film, even I\\'m Mexican and proud of his success in mainstream Hollywood.<br /><br />In another hand, I see most of the reviews focuses on their favorite (and not so) short films; but we are forgetting that there is a subtle bottom line that circles the whole compilation, and maybe it will not be so pleasant for American people. (Even if that was not the main purpose of the producers) <br /><br />What i\\'m talking about is that most of the short films does not show the suffering that WASP people went through because the terrorist attack on September 11th, but the suffering of the Other people.<br /><br />Do you need proofs about what i\\'m saying? Look, in the Bosnia short film, the message is: \"You cry because of the people who died in the Towers, but we (The Others = East Europeans) are crying long ago for the crimes committed against our women and nobody pay attention to us like the whole world has done to you\".<br /><br />Even though the Burkina Fasso story is more in comedy, there is a the same thought: \"You are angry because Osama Bin Laden punched you in an evil way, but we (The Others = Africans) should be more angry, because our people is dying of hunger, poverty and AIDS long time ago, and nobody pay attention to us like the whole world has done to you\".<br /><br />Look now at the Sean Penn short: The fall of the Twin Towers makes happy to a lonely (and alienated) man. So the message is that the Power and the Greed (symbolized by the Towers) must fall for letting the people see the sun rise and the flowers blossom? It is remarkable that this terrible bottom line has been proposed by an American. There is so much irony in this short film that it is close to be subversive.<br /><br />Well, the Ken Loach (very know because his anti-capitalism ideology) is much more clearly and shameless in going straight to the point: \"You are angry because your country has been attacked by evil forces, but we (The Others = Latin Americans) suffered at a similar date something worst, and nobody remembers our grief as the whole world has done to you\".<br /><br />It is like if the creative of this project wanted to say to Americans: \"You see now, America? You are not the only that have become victim of the world violence, you are not alone in your pain and by the way, we (the Others = the Non Americans) have been suffering a lot more than you from long time ago; so, we are in solidarity with you in your pain... and by the way, we are sorry because you have had some taste of your own medicine\" Only the Mexican and the French short films showed some compassion and sympathy for American people; the others are like a slap on the face for the American State, that is not equal to American People.'\n",
            "1\n",
            "b'Blood Castle (aka Scream of the Demon Lover, Altar of Blood, Ivanna--the best, but least exploitation cinema-sounding title, and so on) is a very traditional Gothic Romance film. That means that it has big, creepy castles, a headstrong young woman, a mysterious older man, hints of horror and the supernatural, and romance elements in the contemporary sense of that genre term. It also means that it is very deliberately paced, and that the film will work best for horror mavens who are big fans of understatement. If you love films like Robert Wise\\'s The Haunting (1963), but you also have a taste for late 1960s/early 1970s Spanish and Italian horror, you may love Blood Castle, as well.<br /><br />Baron Janos Dalmar (Carlos Quiney) lives in a large castle on the outskirts of a traditional, unspecified European village. The locals fear him because legend has it that whenever he beds a woman, she soon after ends up dead--the consensus is that he sets his ferocious dogs on them. This is quite a problem because the Baron has a very healthy appetite for women. At the beginning of the film, yet another woman has turned up dead and mutilated.<br /><br />Meanwhile, Dr. Ivanna Rakowsky (Erna Sch\\xc3\\xbcrer) has appeared in the center of the village, asking to be taken to Baron Dalmar\\'s castle. She\\'s an out-of-towner who has been hired by the Baron for her expertise in chemistry. Of course, no one wants to go near the castle. Finally, Ivanna finds a shady individual (who becomes even shadier) to take her. Once there, an odd woman who lives in the castle, Olga (Cristiana Galloni), rejects Ivanna and says that she shouldn\\'t be there since she\\'s a woman. Baron Dalmar vacillates over whether she should stay. She ends up staying, but somewhat reluctantly. The Baron has hired her to try to reverse the effects of severe burns, which the Baron\\'s brother, Igor, is suffering from.<br /><br />Unfortunately, the Baron\\'s brother appears to be just a lump of decomposing flesh in a vat of bizarre, blackish liquid. And furthermore, Ivanna is having bizarre, hallucinatory dreams. Just what is going on at the castle? Is the Baron responsible for the crimes? Is he insane? <br /><br />I wanted to like Blood Castle more than I did. As I mentioned, the film is very deliberate in its pacing, and most of it is very understated. I can go either way on material like that. I don\\'t care for The Haunting (yes, I\\'m in a very small minority there), but I\\'m a big fan of 1960s and 1970s European horror. One of my favorite directors is Mario Bava. I also love Dario Argento\\'s work from that period. But occasionally, Blood Castle moved a bit too slow for me at times. There are large chunks that amount to scenes of not very exciting talking alternated with scenes of Ivanna slowly walking the corridors of the castle.<br /><br />But the atmosphere of the film is decent. Director Jos\\xc3\\xa9 Luis Merino managed more than passable sets and locations, and they\\'re shot fairly well by Emanuele Di Cola. However, Blood Castle feels relatively low budget, and this is a Roger Corman-produced film, after all (which usually means a low-budget, though often surprisingly high quality \"quickie\"). So while there is a hint of the lushness of Bava\\'s colors and complex set decoration, everything is much more minimalist. Of course, it doesn\\'t help that the Retromedia print I watched looks like a 30-year old photograph that\\'s been left out in the sun too long. It appears \"washed out\", with compromised contrast.<br /><br />Still, Merino and Di Cola occasionally set up fantastic visuals. For example, a scene of Ivanna walking in a darkened hallway that\\'s shot from an exaggerated angle, and where an important plot element is revealed through shadows on a wall only. There are also a couple Ingmar Bergmanesque shots, where actors are exquisitely blocked to imply complex relationships, besides just being visually attractive and pulling your eye deep into the frame.<br /><br />The performances are fairly good, and the women--especially Sch\\xc3\\xbcrer--are very attractive. Merino exploits this fact by incorporating a decent amount of nudity. Sch\\xc3\\xbcrer went on to do a number of films that were as much soft corn porn as they were other genres, with English titles such as Sex Life in a Woman\\'s Prison (1974), Naked and Lustful (1974), Strip Nude for Your Killer (1975) and Erotic Exploits of a Sexy Seducer (1977). Blood Castle is much tamer, but in addition to the nudity, there are still mild scenes suggesting rape and bondage, and of course the scenes mixing sex and death.<br /><br />The primary attraction here, though, is probably the story, which is much a slow-burning romance as anything else. The horror elements, the mystery elements, and a somewhat unexpected twist near the end are bonuses, but in the end, Blood Castle is a love story, about a couple overcoming various difficulties and antagonisms (often with physical threats or harms) to be together.'\n",
            "1\n",
            "b\"I was talked into watching this movie by a friend who blubbered on about what a cute story this was.<br /><br />Yuck.<br /><br />I want my two hours back, as I could have done SO many more productive things with my time...like, for instance, twiddling my thumbs. I see nothing redeeming about this film at all, save for the eye-candy aspect of it...<br /><br />3/10 (and that's being generous)\"\n",
            "0\n",
            "b\"Michelle Rodriguez is the defining actress who could be the charging force for other actresses to look out for. She has the audacity to place herself in a rarely seen tough-girl role very early in her career (and pull it off), which is a feat that should be recognized. Although her later films pigeonhole her to that same role, this film was made for her ruggedness.<br /><br />Her character is a romanticized student/fighter/lover, struggling to overcome her disenchanted existence in the projects, which is a little overdone in film...but not by a girl. That aspect of this film isn't very original, but the story goes in depth when the heated relationships that this girl has to deal with come to a boil and her primal rage takes over.<br /><br />I haven't seen an actress take such an aggressive stance in movie-making yet, and I'm glad that she's getting that original twist out there in Hollywood. This film got a 7 from me because of the average story of ghetto youth, but it has such a great actress portraying a rarely-seen role in a minimal budget movie. Great work.\"\n",
            "1\n",
            "b\"This is one of three 80's movies that I can think of that were sadly overlooked at the time and unfortunately, still overlooked. One of the others was Clownhouse directed by Victor Salva, a movie horribly overlook due to Salva's legal/sexual problems. Another would be Cameron's Closet which strikes me as somewhat underrated--not great, but not nearly as bad as the reviews I've seen. Paper House is well worth your time and I think that it is one of those very quiet films that will just stick in your brain for far longer than you might think. I mean, 10 years after I've seen it and I still give it some pause, whereas something that I might have seen 6 months ago has gone into the ether.\"\n",
            "1\n",
            "b'We bought the DVD set of \"Es war einmal das Leben\" (German) / \"Once Upon a Time... Life\" (English) for our bilingual kids because everyone loved the \"Es war einmal der Mensch\" (German) / \"Once Upon a Time... Man\" (English) series (us parents had seen it as kids) and it has exceeded even high expectations! The series is very well made, does not show its age, and our kids at various ages really like to watch it. At the same time, they learn things us parents didn\\'t know until way, way later. The series covers everything to do with the human body from organs, all senses, blood, infection, antibodies, and much more in animated 20-25 min episodes. Topics some people may find \"sensible\", such as digestion and reproduction are covered in a tasteful, discreet and child-friendly manner (the reproduction episode starts coverage mainly where the baby starts growing), while still (as typical) informative and fun.<br /><br />Children are usually fascinated with how their bodies work and through the episodes gain an understanding of this in the context of their environment. The format of the episodes switches between the outside world (a family with 2 children) and the inside of the body. For example, in the episode covering infections, the boy cuts himself accidentally and the wound gets infected and the episode covers how the body reacts to this. Similarly, the episodes on the senses, e.g. hearing, seeing, link what happens inside the body to the context of the outside world and the episode on respiration and circulation of oxygen in the blood covers the complete lifecycle including (briefly) where the oxygen comes from (plants).<br /><br />This is one of the best ever children\\'s programs - I would say it\\'s a must see for every family with kids!'\n",
            "1\n",
            "b'This one came out during the Western genre\\xe2\\x80\\x99s last gasp; unfortunately, it emerges to be a very minor and altogether unsatisfactory effort \\xe2\\x80\\x93 even if made by and with veterans in the field! To begin with, the plot offers nothing remotely new: James Coburn escapes from a chain gang, intent on killing the man (now retired) who put him there \\xe2\\x80\\x93 Charlton Heston. While the latter lays a trap for him, Coburn outwits Heston by kidnapping his daughter (Barbara Hershey). Naturally, the former lawman \\xe2\\x80\\x93 accompanied by Hershey\\xe2\\x80\\x99s greenhorn fianc\\xc3\\xa9 (Chris Mitchum) \\xe2\\x80\\x93 sets out in pursuit of Coburn and his followers, all of whom broke jail along with him.<br /><br />Rather than handling the proceedings in his customary sub-Fordian style, McLaglen goes for a Sam Peckinpah approach \\xe2\\x80\\x93 with which he\\xe2\\x80\\x99s never fully at ease: repellent characters, plenty of violence, and the sexual tension generated by Hershey\\xe2\\x80\\x99s presence among Coburn\\xe2\\x80\\x99s lusty bunch. Incidentally, Heston and Coburn had previously appeared together in a Sam Peckinpah Western \\xe2\\x80\\x93 the troubled MAJOR DUNDEE (1965; I really need to pick up the restored edition of this one on DVD, though I recently taped the theatrical version in pan-and-scan format off TCM UK). Anyway, the film is too generic to yield the elegiac mood it clearly strives for (suggested also by the title): then again, both stars had already paid a fitting valediction to this most American of genres \\xe2\\x80\\x93 WILL PENNY (1968) for Heston and Coburn with PAT GARRETT & BILLY THE KID (1973)! <br /><br />At least, though, Heston maintains a modicum of dignity here \\xe2\\x80\\x93 his ageing character attempting to stay ahead of half-breed Coburn by anticipating what his next move will be; the latter, however, tackles an uncommonly brutish role and only really comes into his own at the climax (relishing his moment of vengeance by sadistically forcing Heston to witness his associates\\xe2\\x80\\x99 gang-rape of Hershey). Apart from the latter, this lengthy sequence sees Heston try to fool Coburn with a trick borrowed from his own EL CID (1961), the villainous gang is then trapped inside a bushfire ignited by the practiced Heston and the violent death of the two \\xe2\\x80\\x98obsolete\\xe2\\x80\\x99 protagonists (as was his fashion, Heston\\xe2\\x80\\x99s demise takes the form of a gratuitous sacrifice!).<br /><br />The supporting cast includes Michael Parks as the ineffectual town sheriff, Jorge Rivero as Coburn\\xe2\\x80\\x99s Mexican lieutenant, and Larry Wilcox \\xe2\\x80\\x93 of the TV series CHiPs! \\xe2\\x80\\x93 as the youngest member of Coburn\\xe2\\x80\\x99s gang who\\xe2\\x80\\x99s assigned the task of watching over Hershey (while doing his best to keep his drooling mates away!). Jerry Goldsmith contributes a flavorful but, at the same time, unremarkable score.'\n",
            "0\n",
            "b\"I used to LOVE this movie as a kid but, seeing it again 20+ years later, it actually sucks. Up The Academy might have been ahead of it's time back in 1980, but it has almost nothing to offer today! Movies like Caddyshack and Stripes hold-up much better today than this steaming dogpile. No T&A. No great jokes except for the one-liners we've all heard a million times by now.<br /><br />I recently bought the DVD in hopes that it would be the gem I remembered it being. Well, I was WAY off! The soundtrack had only 2-3 widely-recognizable hits (not the smash compilation others had mentioned) and the frequent voice-overs were terrible. The only thing that was interesting, to me, was predicting what the character's lines were before they said them. Yep, I watched this movie that much back then! <br /><br />The only reason I am writing this review is to give my two cents on why this movie should be forgotten, sorry to say. :(\"\n",
            "0\n",
            "b'I just don\\'t see how a Concorde-New Horizons film directed by Jim Wynorski and featuring the acting talents of Andrew Stevens and a puppet could be bad. It just boggles the mind, doesn\\'t it?<br /><br />Well, let\\'s make no mistake about it. \"Munchie Strikes Back\" is indeed a bad film. Munchie is a puppet who has been around for many centuries. For reasons not fully explained until the end of the film, he is sent to Earth to help a single mother and her son. The mom\\'s problem (her main problem at least) is that she has a balloon payment due on her mortgage in two weeks...to the not-so-tiny tune of $20,000. Ouch. She can\\'t come up with the money because she just got fired. OK...JUST is the key word in that sentence. What the...? Was she planning on paying it off with a single paycheck? Maybe it would\\'ve been a good idea to have spent the last several years saving up for it...ya think?<br /><br />Munchie has magical powers similar to those a genie would possess...but there isn\\'t a limit on the number of wishes you can make! Munchie gets the boy a bunch of fancy stuff for one night but then the kid asks for it to be sent back to the mall Munchie was \"borrowing\" it from. The annoying furball also uses his otherworldly skills to help the boy win a baseball game by means of cheating. A baseball is hit so hard that it orbits the Earth several times. Sadly, those dumb parents watching the game don\\'t think it\\'s at all strange. Hmm.<br /><br />Anyway, I\\'d like to wrap this up because this has already drained away enough of my lifeforce as it is. You\\'ll be truly moved by the scene where Leslie-Anne Down, playing the mother, kicks a dog which is yapping at her. Your heart will melt at her charm when she notices dollar bills fluttering down on her front yard and she wonders how it could be snowing during the summer. \"Munchie Strikes Back\"\\'s credits promised another film to follow entitled, I believe, \"Munchie Hangs Ten\". To date, the movie viewing public has been robbed of what would surely have been a cinematic tour de force. Heh. 1/10'\n",
            "0\n",
            "b'This film is so unbelievable; - the whole premise is bunkum; the fact that a serial killer (vampire or otherwise) could fly around untraced and kill as many people as the film implies is laughable. The vampire himself would not look out of place in a Bela Lugosi film. Most of the acting is so wooden the actors should be treated for dry rot. I await the day when someone makes a decent film from a Steven King novel (with the exception, possibly, of Stand by Me). This film suffers from what most Stephen King films do - lack of money used for the \"special\" effects, poor actors, appalling characterisation and dialogue. This film is cheap, tacky and fails in everything it tries to do.'\n",
            "0\n",
            "b'I admit the problem I have with the much-celebrated Ealing films I\\'ve seen so far could be mine. To my taste, either they are black and rippingly funny, or so light in tone to be unsatisfying as comedies or stories. That\\'s a self-important way of saying I wanted to like \"The Man In The White Suit\" but found myself struggling to sit through its short run time.<br /><br />Textile worker Sidney Stratton (Alec Guinness) may be meek in manner, but he is doggedly committed to progress in the form of his attempts to invent a strand of fabric that can\\'t be broken or made dirty. Using a factory lab for his latest experiment, he toils against limitations both material and human - the latter being the benighted mill bosses who don\\'t understand what he is up to, then figure it out and become even more committed to stopping him.<br /><br />\"It\\'s small minds like yours that stand in the way of progress,\" Sidney complains, practicing in the mirror what he struggles to say to the Man.<br /><br />One problem with \"Man In The White Suit\" is that Sidney\\'s vision of progress is awfully small-minded, too, more so even than that of the bosses or the laborers who also resent his work. My problem is more elemental: For a comedy, \"White Suit\" is not funny. It\\'s a rather earnest script which too often tries to mine its feeble attempts at humor from spit-takes, double-takes, triple-takes, and dizzy takes.<br /><br />The best joke is the sound of the machine Sidney toils at, going \"Bleep-Blop-Bleep-Bop\" endlessly and fetching queer looks from every visitor until Sidney either extracts his miracle from it or blows it up trying. Like every other bit of stray humor that functions decently in this film, it\\'s leaned on too long.<br /><br />I\\'ve never seen Guinness less affecting in a movie, even though he looks impossibly young and earnest (though actually in his mid-30s). He seems so bloodless, even more so than the wax-faced general he played in \"Doctor Zhivago\" He\\'s the same cold fish whether he\\'s ignoring the sad affections of the affecting mill girl who offers to give him her life savings when he loses his job (pan-faced Vida Hope as Bertha) or the more sultry charms of young Daphne Birnley (Joan Greenwood), his one real ally in his fight against \"shabbiness and dirt\", as she puts it, making those words sound as impossibly sexy as only Greenwood could.<br /><br />Supporting players make \"White Suit\" work as well as it does. Ernest Thesinger of \"Bride Of Frankenstein\" fame plays a singularly nasty captain of industry who looks like Nosferatu and makes a laugh like a death rattle. Howard Marion-Crawford as another factory leader is as memorable here as he was playing a blinkered medical officer in \"Lawrence Of Arabia\". Then there\\'s the undeniable charm of Mandy Miller as a little girl who steals her few moments on camera right from under everyone else.<br /><br />But most of the scenes are played so straight that one wouldn\\'t think director Alexander Mackendrick had ever worked on a comedy before (his previous Ealing comedy \"Whisky Galore\" doesn\\'t reverse that impression, alas). Roger MacDougall\\'s play posits the notion of scientific progress as potential disaster, but fails to present dull Sidney in anything other than the most blandly pleasant of lights.<br /><br />Ealing comedies are remembered for capturing the human side of comedy. Yet the Ealings I\\'ve seen never seem to do this, working only when they play aggressively against our own sympathies. \"Kind Hearts And Coronets\" and \"The Ladykillers\" (Mackendrick again, go figure) are classics this way. \"White Suit\", on the other hand, is a pointless ramble that falls apart when it should cohere, just like that unfortunate suit.'\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing the data\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import string\n",
        "import re\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )"
      ],
      "metadata": {
        "id": "upD598h7-yPC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_ds = raw_train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "-AyG4ETb--e5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "QPJMQkwa_EYz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the data.\n",
        "train_data = raw_train_data.map(vectorize_text)\n",
        "val_data = raw_val_data.map(vectorize_text)\n",
        "test_data = raw_test_data.map(vectorize_text)"
      ],
      "metadata": {
        "id": "fIB-KnjU_ZYu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform  prefetch buffering on the data.\n",
        "train_data = train_data.cache().prefetch(buffer_size=10)\n",
        "val_data = val_data.cache().prefetch(buffer_size=10)\n",
        "test_data = test_data.cache().prefetch(buffer_size=10)\n"
      ],
      "metadata": {
        "id": "iMb_8Rx7_i0O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "4-3vKLYg_43M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "embedding_dim = 64\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "filters = 128\n",
        "kernel_size = 5\n",
        "x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
        "x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "learning_rate = 1e-4\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqxKh0uNFQwj",
        "outputId": "2d82f552-88a5-4d18-fc2e-32399d9c75ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "epochs = 3\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_data, validation_data=val_data, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBWTPyMtFa4L",
        "outputId": "24e01443-cc5f-4319-d4f3-427b710a4635"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 176s 278ms/step - loss: 0.5606 - accuracy: 0.6644 - val_loss: 0.3639 - val_accuracy: 0.8434\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 168s 269ms/step - loss: 0.3026 - accuracy: 0.8745 - val_loss: 0.2984 - val_accuracy: 0.8770\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 170s 272ms/step - loss: 0.1917 - accuracy: 0.9274 - val_loss: 0.3167 - val_accuracy: 0.8786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f580e47fc10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "requE03OIYfy",
        "outputId": "23946b08-19eb-4df1-e28b-1075711bbea2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 59s 75ms/step - loss: 0.3285 - accuracy: 0.8720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3285134434700012, 0.871999979019165]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvU9v4g7HxI"
      },
      "source": [
        "Task 2 : Seq2seq is a family of machine-learning approaches used for natural language processing. Applications include language translation, image captioning, conversational models, and text summarization. \"ted_multi_translate\" is a multilingual (60 language) data set derived from TED Talk transcripts. Construct a seq2seq model, train it with \"tec_multi_translate\" training data split, and report your model performance with the test data split. You can use various techniques, such as LSTM, transformers, attention, and pretraining/fine-tuning. This task is open-ended. Useful links:\n",
        "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "https://www.tensorflow.org/datasets/catalog/ted_multi_translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u93T6_h-7Mb2"
      },
      "outputs": [],
      "source": [
        "#Import the packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aaRoh2S7URy",
        "outputId": "c5a802ab-71ea-4808-c0ec-3f897d474495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  335M  100  335M    0     0  32.2M      0  0:00:10  0:00:10 --:--:-- 35.4M\n"
          ]
        }
      ],
      "source": [
        "!curl -O http://phontron.com/data/ted_talks.tar.gz\n",
        "!tar -xf ted_talks.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RCmAl-qL7bJe"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "training_data = pd.read_csv('all_talks_train.tsv', sep='\\t').head(1000)\n",
        "testing_data = pd.read_csv('all_talks_test.tsv', sep='\\t').head(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a-o9UbYW8CHc"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "training_data[\"en\"] = training_data[\"en\"].str.lower()\n",
        "training_data[\"de\"] = training_data[\"de\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cIhRYKRD8QqA"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text\n",
        "english_tokenize = Tokenizer()\n",
        "english_tokenize.fit_on_texts(training_data[\"en\"])\n",
        "german_tokenize = Tokenizer()\n",
        "german_tokenize.fit_on_texts(training_data[\"de\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2mi5xm6X-nES"
      },
      "outputs": [],
      "source": [
        "# Convert text to sequences\n",
        "english_seq = english_tokenize.texts_to_sequences(training_data[\"en\"])\n",
        "german_seq = german_tokenize.texts_to_sequences(training_data[\"de\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_3pIEUdm_iZn"
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to a fixed length\n",
        "max_len = 100\n",
        "fixed_english_seq = pad_sequences(english_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "fixed_german_seq = pad_sequences(german_seq, maxlen=max_len + 1, padding=\"post\", truncating=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S3ooi8GAAI6P"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "test_size = 0.2\n",
        "english_train, english_test, german_train, german_test = train_test_split(\n",
        "    fixed_english_seq, fixed_german_seq, test_size=test_size, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DJTh0nT2ArBV"
      },
      "outputs": [],
      "source": [
        "# Define input and output sequence length\n",
        "input_seq_len = english_train.shape[1]\n",
        "output_seq_len = english_train.shape[1]\n",
        "\n",
        "# Define vocabulary size for English and French\n",
        "vocab_size_english = len(english_tokenize.word_index) + 1\n",
        "vocab_size_german = len(german_tokenize.word_index) + 1\n",
        "\n",
        "# Define input layer\n",
        "input_layer = Input(shape=(input_seq_len,))\n",
        "\n",
        "# Define embedding size\n",
        "embedding_size = 256\n",
        "\n",
        "# Define embedding layer for encoder\n",
        "encoder_embedding = Embedding(vocab_size_english, embedding_size, mask_zero=True)(input_layer)\n",
        "\n",
        "# Define LSTM layer for encoder\n",
        "lstm_layer_encoder = LSTM(256, return_state=True)\n",
        "\n",
        "# Get encoder outputs and states\n",
        "encoder_outputs, state_h, state_c = lstm_layer_encoder(encoder_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dOuP5gHRCq_W"
      },
      "outputs": [],
      "source": [
        "#Define input layer for decoder\n",
        "decoder_inputs = Input(shape=(output_seq_len,))\n",
        "\n",
        "#Define embedding layer for decoder\n",
        "decoder_embedding = Embedding(vocab_size_german, 256, mask_zero=True)(decoder_inputs)\n",
        "\n",
        "#Define LSTM layer for decoder\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "\n",
        "#Deine decoder outputs and states\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "#define decoder_dense with softmax activation\n",
        "decoder_dense = Dense(vocab_size_german, activation='softmax')\n",
        "\n",
        "#define outputs \n",
        "outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JozOSSjHEch2",
        "outputId": "09c60870-69eb-4a4f-db47-260aba51f037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 100, 256)     804096      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 100, 256)     990720      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        525312      ['embedding_2[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 100, 256),   525312      ['embedding_3[0][0]',            \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100, 3870)    994590      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,840,030\n",
            "Trainable params: 3,840,030\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#define model\n",
        "model = Model([input_layer, decoder_inputs], outputs)\n",
        "\n",
        "#compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "#print model summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GylRF6IeFe7F",
        "outputId": "51827d75-d0ff-4a16-c963-3833c6c2b9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 47s 10s/step - loss: 8.2589 - val_loss: 8.2503\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 34s 8s/step - loss: 8.2382 - val_loss: 8.1215\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 31s 8s/step - loss: 7.9238 - val_loss: 7.4238\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 31s 7s/step - loss: 7.1423 - val_loss: 6.8801\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 32s 8s/step - loss: 6.6499 - val_loss: 6.8984\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 34s 8s/step - loss: 6.6337 - val_loss: 7.0433\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 33s 8s/step - loss: 6.6293 - val_loss: 7.0595\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 31s 8s/step - loss: 6.5485 - val_loss: 7.0723\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 31s 7s/step - loss: 6.5141 - val_loss: 7.1121\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 35s 8s/step - loss: 6.4985 - val_loss: 7.1480\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57f99697e0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#define batch_size and epochs\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "#train the model using english and german training sets and validate with english and german testing sets\n",
        "model.fit([english_train, german_train[:, :-1]],german_train[:, 1:],batch_size=batch_size,epochs=epochs,validation_data=([english_test, german_test[:, :-1]], german_test[:, 1:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9JRW-xuHtGw",
        "outputId": "0fd42f83-6c03-4519-a468-6f7ad492ba4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 5s 676ms/step - loss: 7.1454\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.1454033851623535"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#evaluate the model\n",
        "model.evaluate([english_test, german_test[:, :-1]], german_test[:, 1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRurCumdKRSR",
        "outputId": "728cdf20-01df-4746-c19c-4085fe648dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 5s 393ms/step\n",
            "BLEU score: 0.8428206297233874\n",
            "Accuracy: 0.03\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Predict on test data\n",
        "preds = model.predict([english_test, german_test[:, :-1]])\n",
        "preds = np.argmax(preds, axis=-1)\n",
        "\n",
        "# Convert integer-encoded predictions to text\n",
        "# Define target_word_index\n",
        "target_word_index = german_tokenize.word_index\n",
        "reverse_target_word_index = {v: k for k, v in target_word_index.items()}\n",
        "preds_text = []\n",
        "for pred in preds:\n",
        "    pred_text = \" \".join([reverse_target_word_index.get(idx, \"<OOV>\") for idx in pred])\n",
        "    preds_text.append(pred_text)\n",
        "\n",
        "# Convert integer-encoded actuals to text\n",
        "actuals_text = []\n",
        "for actual in german_test[:, 1:]:\n",
        "    actual_text = \" \".join([reverse_target_word_index.get(idx, \"<OOV>\") for idx in actual])\n",
        "    actuals_text.append(actual_text)\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu([[actual_text] for actual_text in actuals_text], preds_text)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(np.array(preds_text) == np.array(actuals_text))\n",
        "\n",
        "# Print results\n",
        "print(\"BLEU score:\", bleu_score)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}